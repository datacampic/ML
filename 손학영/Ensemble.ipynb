{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble \n",
    "[참고](https://towardsdatascience.com/ensemble-methods-bagging-boosting-and-stacking-c9214a10a205)\n",
    "> 앙상블 학습법 \n",
    "- 배깅\n",
    "- 부스팅 \n",
    "- 스태킹\n",
    "\n",
    "> 사전 개념 \n",
    "- 부트스트랩 : 모수의 분포를 추정하기 위해 표본에서 추가적으로 표본을 복원 추출하고 각 표본에 대한 통계량을 다시 계산하는 절차 (갤노트)\n",
    "- 결정 트리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. 사전 개념 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>drinks_coffee</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4509</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>False</td>\n",
       "      <td>64.538179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1864</td>\n",
       "      <td>&gt;=21</td>\n",
       "      <td>True</td>\n",
       "      <td>65.824249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2060</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>False</td>\n",
       "      <td>71.319854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7875</td>\n",
       "      <td>&gt;=21</td>\n",
       "      <td>True</td>\n",
       "      <td>68.569404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6254</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>True</td>\n",
       "      <td>64.020226</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id   age  drinks_coffee     height\n",
       "0     4509   <21          False  64.538179\n",
       "1     1864  >=21           True  65.824249\n",
       "2     2060   <21          False  71.319854\n",
       "3     7875  >=21           True  68.569404\n",
       "4     6254   <21           True  64.020226"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 부트스트랩 \n",
    "df = pd.read_csv('./data/coffee_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 커피를 마시는 사람과 안마시는 사람의 키를 비교해보자! \n",
    "- 200개의 샘플에서 부트스트래핑 할 것 \n",
    "- **<p style = \"color : red;\" >커피가 키에 영향을 미치는지 분석</p>**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>drinks_coffee</th>\n",
       "      <th>height</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>3171</td>\n",
       "      <td>&gt;=21</td>\n",
       "      <td>True</td>\n",
       "      <td>70.212902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>436</th>\n",
       "      <td>5633</td>\n",
       "      <td>&gt;=21</td>\n",
       "      <td>True</td>\n",
       "      <td>67.563800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1835</th>\n",
       "      <td>6720</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>False</td>\n",
       "      <td>66.016555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1077</th>\n",
       "      <td>2891</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>False</td>\n",
       "      <td>68.236466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>357</th>\n",
       "      <td>5134</td>\n",
       "      <td>&lt;21</td>\n",
       "      <td>False</td>\n",
       "      <td>63.629469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      user_id   age  drinks_coffee     height\n",
       "46       3171  >=21           True  70.212902\n",
       "436      5633  >=21           True  67.563800\n",
       "1835     6720   <21          False  66.016555\n",
       "1077     2891   <21          False  68.236466\n",
       "357      5134   <21          False  63.629469"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모집단에서 200개의 표본을 랜덤샘플링 \n",
    "\n",
    "df_sample = df.sample(200)\n",
    "df_sample.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 부트스트랩을 10000번 반복해 커피를 마시지 않는 사람과 마시는 사람의 키 차이의 99% 신뢰구간 구하기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-2.794555433332615, -0.44373974132486593)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 커피를 마시지 않는 사람과 커피를 마시는 사람의 평균 키 차이 \n",
    "iteration = 10000\n",
    "differheight = [] # 평균의 차이 10000개가 담기는 곳 \n",
    "for _ in range(iteration) : \n",
    "    bootSample = df_sample.sample(200,replace=True) # 복원추출 \n",
    "    nonCoffeeHeightMean = bootSample[bootSample['drinks_coffee'] == False].height.mean() # 커피를 마시지 않는 사람 평균 키\n",
    "    coffeeHeightMean = bootSample[bootSample['drinks_coffee'] == True].height.mean() # 커피를 마시는 사람 평균 키\n",
    "    diff = nonCoffeeHeightMean - coffeeHeightMean\n",
    "    differheight.append(diff) # 평균의 차이 만들기 \n",
    "\n",
    "#  신뢰수준 99%인 평균 키 차이에 대한 신뢰구간\n",
    "np.percentile(differheight, 0.5), np.percentile(differheight, 99.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21살 이상과 21살 미만인 사람들의 평균 키 차이 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21살 미만인 사람들 중 커피를 마시지 않는 사람과 커피를 마시는 사람의 평균 키 차이 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 21살 이상인 사람들 중 커피를 마시지 않는 사람과 커피를 마시는 사람의 평균 키 차이 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bagging meta-estimator\n",
    "\n",
    "> 배깅이란? \n",
    "- 샘플을 여러 번 뽑아 각 모델을 학습시켜 결과물을 집계하는 방법\n",
    "\n",
    "> 배깅 방법\n",
    "1) 모수로부터 `부트스트랩`(복원 랜덤 샘플링)한 데이터로 모델을 학습 - 중복 허용 \n",
    "2) 학습된 모델의 결과를 집계하여 최종 결과 값 구함 \n",
    "3) Bagging Features - 모델을 만들 때 사용될 속성들을 제한함으로써 각 모델의 다양성을 보장\n",
    "    - 각 모델 당 뽑는 속성의 개수는 전체 속성 개수의 제곱근만큼 선택 \n",
    "\n",
    "> 집계 방법 \n",
    "1) Categorical - 투표 집계  \n",
    "2) Continuous - 평균 집계 \n",
    "\n",
    "> 장점 \n",
    "- 의사결정나무의 단점인 <span style = \"color : red;\">높은 분산이 줄어들어 성능이 향상됨</span> \n",
    "\n",
    "> 대표적인 모델 \n",
    "- 랜덤 포레스트 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 랜덤 포레스트 \n",
    "- 배깅의 일종 \n",
    "- 설명변수 무작위 선택 -> 트리의 다양성을 확보 -> `모형간의 상관관계 감소`\n",
    "- 즉, 서로 상관관계없는 트리를 만들어내어 결과를 평균 및 다수결로 만듦 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 불러오기 \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier = RandomForestClassifier(n_estimators = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "      <th>Type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.52101</td>\n",
       "      <td>13.64</td>\n",
       "      <td>4.49</td>\n",
       "      <td>1.10</td>\n",
       "      <td>71.78</td>\n",
       "      <td>0.06</td>\n",
       "      <td>8.75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.51761</td>\n",
       "      <td>13.89</td>\n",
       "      <td>3.60</td>\n",
       "      <td>1.36</td>\n",
       "      <td>72.73</td>\n",
       "      <td>0.48</td>\n",
       "      <td>7.83</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.51618</td>\n",
       "      <td>13.53</td>\n",
       "      <td>3.55</td>\n",
       "      <td>1.54</td>\n",
       "      <td>72.99</td>\n",
       "      <td>0.39</td>\n",
       "      <td>7.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.51766</td>\n",
       "      <td>13.21</td>\n",
       "      <td>3.69</td>\n",
       "      <td>1.29</td>\n",
       "      <td>72.61</td>\n",
       "      <td>0.57</td>\n",
       "      <td>8.22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.51742</td>\n",
       "      <td>13.27</td>\n",
       "      <td>3.62</td>\n",
       "      <td>1.24</td>\n",
       "      <td>73.08</td>\n",
       "      <td>0.55</td>\n",
       "      <td>8.07</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        RI     Na    Mg    Al     Si     K    Ca   Ba   Fe  Type\n",
       "0  1.52101  13.64  4.49  1.10  71.78  0.06  8.75  0.0  0.0     1\n",
       "1  1.51761  13.89  3.60  1.36  72.73  0.48  7.83  0.0  0.0     1\n",
       "2  1.51618  13.53  3.55  1.54  72.99  0.39  7.78  0.0  0.0     1\n",
       "3  1.51766  13.21  3.69  1.29  72.61  0.57  8.22  0.0  0.0     1\n",
       "4  1.51742  13.27  3.62  1.24  73.08  0.55  8.07  0.0  0.0     1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/glass.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RI</th>\n",
       "      <th>Na</th>\n",
       "      <th>Mg</th>\n",
       "      <th>Al</th>\n",
       "      <th>Si</th>\n",
       "      <th>K</th>\n",
       "      <th>Ca</th>\n",
       "      <th>Ba</th>\n",
       "      <th>Fe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.872868</td>\n",
       "      <td>0.284953</td>\n",
       "      <td>1.254639</td>\n",
       "      <td>-0.692442</td>\n",
       "      <td>-1.127082</td>\n",
       "      <td>-0.671705</td>\n",
       "      <td>-0.145766</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.249333</td>\n",
       "      <td>0.591817</td>\n",
       "      <td>0.636168</td>\n",
       "      <td>-0.170460</td>\n",
       "      <td>0.102319</td>\n",
       "      <td>-0.026213</td>\n",
       "      <td>-0.793734</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.721318</td>\n",
       "      <td>0.149933</td>\n",
       "      <td>0.601422</td>\n",
       "      <td>0.190912</td>\n",
       "      <td>0.438787</td>\n",
       "      <td>-0.164533</td>\n",
       "      <td>-0.828949</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.232831</td>\n",
       "      <td>-0.242853</td>\n",
       "      <td>0.698710</td>\n",
       "      <td>-0.310994</td>\n",
       "      <td>-0.052974</td>\n",
       "      <td>0.112107</td>\n",
       "      <td>-0.519052</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.312045</td>\n",
       "      <td>-0.169205</td>\n",
       "      <td>0.650066</td>\n",
       "      <td>-0.411375</td>\n",
       "      <td>0.555256</td>\n",
       "      <td>0.081369</td>\n",
       "      <td>-0.624699</td>\n",
       "      <td>-0.352877</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>-0.704815</td>\n",
       "      <td>0.898681</td>\n",
       "      <td>-1.865511</td>\n",
       "      <td>2.881125</td>\n",
       "      <td>-0.052974</td>\n",
       "      <td>-0.640968</td>\n",
       "      <td>0.157088</td>\n",
       "      <td>1.783978</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>-0.500178</td>\n",
       "      <td>1.856097</td>\n",
       "      <td>-1.865511</td>\n",
       "      <td>1.094342</td>\n",
       "      <td>0.529374</td>\n",
       "      <td>-0.763919</td>\n",
       "      <td>-0.392276</td>\n",
       "      <td>2.852405</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>0.754046</td>\n",
       "      <td>1.168721</td>\n",
       "      <td>-1.865511</td>\n",
       "      <td>1.154570</td>\n",
       "      <td>0.995252</td>\n",
       "      <td>-0.763919</td>\n",
       "      <td>-0.364103</td>\n",
       "      <td>2.953200</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>-0.612399</td>\n",
       "      <td>1.193270</td>\n",
       "      <td>-1.865511</td>\n",
       "      <td>0.993960</td>\n",
       "      <td>1.241133</td>\n",
       "      <td>-0.763919</td>\n",
       "      <td>-0.335931</td>\n",
       "      <td>2.812087</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>-0.414363</td>\n",
       "      <td>1.009152</td>\n",
       "      <td>-1.865511</td>\n",
       "      <td>1.275028</td>\n",
       "      <td>0.917606</td>\n",
       "      <td>-0.763919</td>\n",
       "      <td>-0.237327</td>\n",
       "      <td>3.013677</td>\n",
       "      <td>-0.586451</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>214 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           RI        Na        Mg        Al        Si         K        Ca  \\\n",
       "0    0.872868  0.284953  1.254639 -0.692442 -1.127082 -0.671705 -0.145766   \n",
       "1   -0.249333  0.591817  0.636168 -0.170460  0.102319 -0.026213 -0.793734   \n",
       "2   -0.721318  0.149933  0.601422  0.190912  0.438787 -0.164533 -0.828949   \n",
       "3   -0.232831 -0.242853  0.698710 -0.310994 -0.052974  0.112107 -0.519052   \n",
       "4   -0.312045 -0.169205  0.650066 -0.411375  0.555256  0.081369 -0.624699   \n",
       "..        ...       ...       ...       ...       ...       ...       ...   \n",
       "209 -0.704815  0.898681 -1.865511  2.881125 -0.052974 -0.640968  0.157088   \n",
       "210 -0.500178  1.856097 -1.865511  1.094342  0.529374 -0.763919 -0.392276   \n",
       "211  0.754046  1.168721 -1.865511  1.154570  0.995252 -0.763919 -0.364103   \n",
       "212 -0.612399  1.193270 -1.865511  0.993960  1.241133 -0.763919 -0.335931   \n",
       "213 -0.414363  1.009152 -1.865511  1.275028  0.917606 -0.763919 -0.237327   \n",
       "\n",
       "           Ba        Fe  \n",
       "0   -0.352877 -0.586451  \n",
       "1   -0.352877 -0.586451  \n",
       "2   -0.352877 -0.586451  \n",
       "3   -0.352877 -0.586451  \n",
       "4   -0.352877 -0.586451  \n",
       "..        ...       ...  \n",
       "209  1.783978 -0.586451  \n",
       "210  2.852405 -0.586451  \n",
       "211  2.953200 -0.586451  \n",
       "212  2.812087 -0.586451  \n",
       "213  3.013677 -0.586451  \n",
       "\n",
       "[214 rows x 9 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 표준화 \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df_scaledx = pd.DataFrame(scaler.fit_transform(df.drop('Type', axis = 1)), columns = df.columns[:-1])\n",
    "df_scaledx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train, test split \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(df_scaledx, df['Type'], test_size = 0.3, random_state = 1004)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((149, 9), (65, 9), (149,), (65,))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307692307692307"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 훈련 \n",
    "from sklearn.metrics import accuracy_score \n",
    "clf = RandomForestClassifier(n_estimators=20, max_depth=5, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predict = clf.predict(x_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6307692307692307"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample의 개수 증가 \n",
    "clf = RandomForestClassifier(n_estimators=30, max_depth=5, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predict = clf.predict(x_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7076923076923077"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 깊이 증가\n",
    "clf = RandomForestClassifier(n_estimators=30, max_depth=7, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predict = clf.predict(x_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 엔트로피 지수 활용 \n",
    "clf = RandomForestClassifier(criterion = 'entropy', n_estimators=30, max_depth=7, random_state=0)\n",
    "clf.fit(x_train, y_train)\n",
    "\n",
    "predict = clf.predict(x_test)\n",
    "accuracy_score(y_test, predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Boosting \n",
    "\n",
    "> 부스팅이란? \n",
    "- 가중치를 활용하여 분류기 만드는 방법 \n",
    "\n",
    "> 부스팅 방법 \n",
    "1) 모델들이 결과를 예측 - 앞의 모델이 예측하면 그 예측 결과에 따라 데이터에 **가중치가 부여되고 해당 가중치가 다음 모델에 영향 줌.**  \n",
    "2) 잘못 분류된 데이터에 집중하여 새로운 분류 규칙을 만드는 단계를 반복 = 재표본 과정에서 `분류가 잘못된 데이터`에 대해서 더 큰 `가중치/확률`을 부여하여 표본을 추출\n",
    "3) 잘못 분류의 의미 = 잔차를 확인하는 것 \n",
    "4) `잔차를 줄이는 방향`으로 가중치/확률을 부여하는 것 \n",
    "\n",
    "> 중요한 점\n",
    "- 한 스텝에서 계속 개선되어짐 \n",
    "- 그래서 학습 속도 느림 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. AdaBoost \n",
    "- 간단한 약분류기들이 상호보완하도록 단계적으로 학습 \n",
    "- 이를 조합하여 최종 분류기의 성능을 증폭 \n",
    "- 차이점 : 학습에 이용된 개별 분류기에 서로 다른 가중치를 주어서 최종분류기 만듦 \n",
    "\n",
    "> 방법 \n",
    " - 약분류기들을 한 번에 하나씩 순차적으로 학습시킬 때, 먼저 학습된 분류기가 **잘못 분류한 결과정보를 다음 분류기의 학습 시 사용**하여 이전 분류기의 단점을 보완하도록 한다 \n",
    "\n",
    "> Parameter \n",
    " - number of trees : 트리의 개수, cross-validation을 통해 선택\n",
    " - Shrinkage parameter : boosting 알고리즘이 학습하는 속도를 조절하는 파라미터로 0.01, 0.001을 주로 사용 \n",
    " - number of splits : 트리의 크기 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36923076923076925"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 1), # week classifier\n",
    "    n_estimators = 50, # 몇 개의 분류기를 학습할 것인지 \n",
    "    learning_rate= 1 \n",
    ")\n",
    "clf.fit(x_train,y_train)\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36923076923076925"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(n_estimators= 50)\n",
    "clf.fit(x_train,y_train)\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36923076923076925"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(base_estimator = DecisionTreeClassifier(max_depth = 1), # week classifier\n",
    "    n_estimators = 100, # 몇 개의 분류기를 학습할 것인지 \n",
    "    learning_rate= 1 \n",
    ")\n",
    "clf.fit(x_train,y_train)\n",
    "predict = clf.predict(x_test)\n",
    "\n",
    "accuracy_score(y_test, predict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-2. Gradient Boosting\n",
    "> Gradient Descent란? \n",
    "- Gradient Descent 기법을 이용하여 손실함수를 최소화하는 방향으로 학습 \n",
    "\n",
    "> 단점\n",
    "- 느리다 \n",
    "- 과적합 이슈 있다\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6923076923076923"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(loss = 'deviance',\n",
    "                           learning_rate = 0.1,\n",
    "                           n_estimators = 100,\n",
    "                           max_depth = 3)\n",
    "gbc.fit(x_train, y_train)\n",
    "predict = gbc.predict(x_test)\n",
    "accuracy_score(y_test,predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7076923076923077"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정보획득량 기준 변경\n",
    "gbc = GradientBoostingClassifier(loss = 'deviance',\n",
    "                           learning_rate = 0.1,\n",
    "                           n_estimators = 100,\n",
    "                           max_depth = 3,\n",
    "                           criterion= 'mse')\n",
    "gbc.fit(x_train, y_train)\n",
    "predict = gbc.predict(x_test)\n",
    "accuracy_score(y_test,predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-3. XGBoost\n",
    "> GradientBoosting과 차이점\n",
    "- CART 기반의 트리 사용 -> 그래서 분류, 회귀 둘 다 가능\n",
    "- 손실함수뿐만 아니라 모형 복잡도까지 고려\n",
    "- 파라미터 tunning 필수\n",
    "- 하지만 GBM 기반\n",
    "\n",
    "> 장점 \n",
    "- GBM 대비 빠른 수행 시간 : 병렬 처리 \n",
    "- GBM의 과적합 규제 \n",
    "- 분류와 회귀영역에서 뛰어난 예측 성능 발휘 \n",
    "- Early Stop 기능이 있음 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 잠깐!! CART 알고리즘이란? \n",
    "[참고](https://tyami.github.io/machine%20learning/decision-tree-4-CART/)\n",
    "\n",
    "- Classification And Regression Tree의 약자\n",
    "cf) ID3 알고리즘 \n",
    "\n",
    "> 파라미터 \n",
    "- 지니계수 : 분류가 잘 될 때 낮은 값 가짐 -> CART 알고리즘에서는 모든 조합에 대해 지니계수 계산 후 가장 낮은 지표를 찾아 split 한다 \n",
    "- Binary tree : 단 두 개의 노드로 split한다 -> 가지 생성을 `하나의 클래스와 나머지`와 같이 생성됨\n",
    "- regression 지원 : 실제값과 예측값의 오차를 사용 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.2-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\lg\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\lg\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.2\n"
     ]
    }
   ],
   "source": [
    "! pip install xgboost\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LG\\anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[16:50:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6615384615384615"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import plot_importance\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb= XGBClassifier(learning_rate = 0.1, max_depth = 4)\n",
    "xgb.fit(x_train, y_train)\n",
    "pred = xgb.predict(x_test)\n",
    "accuracy_score(y_test, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Stacking \n",
    "\n",
    "> 스태킹이란? \n",
    "- 개별 알고리즘이 예측한 데이터를 기반으로 다시 예측을 수행하는 방법 \n",
    "\n",
    "> 스태킹 방법 \n",
    "1) 개별 알고리즘이 예측 (`기반 모델`)\n",
    "2) 해당 결과 모아서 최종 메타 데이터 셋를 만듦  \n",
    "3) 그 데이터 셋으로 별도의 ML 알고리즘으로 최종학습 수행 (`메타 모델`) \n",
    "4) 테스트 데이터를 기반으로 최종 예측 수행 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# 메타 모델을 위한 학습 및 테스트 데이터 만들기\n",
    "def get_stacking_base_datasets(model, X_train_n, y_train_n, X_test_n, n_folds):\n",
    "    kf = KFold(n_splits=n_folds, shuffle=False, random_state=11)\n",
    "    # 빈 배열 생성\n",
    "    train_fold_pred = np.zeros((X_train_n.shape[0],1))\n",
    "    test_pred = np.zeros((X_test_n.shape[0],n_folds))\n",
    "    \n",
    "    \n",
    "    for folder_counter, (train_index, valid_index) in enumerate(kf.split(X_train_n)):\n",
    "        print('폴드 세트 : ', folder_counter, ' 시작')\n",
    "        X_tr = X_train_n[train_index]\n",
    "        y_tr = y_train_n[train_index]\n",
    "        X_te = X_train_n[valid_index] \n",
    "        \n",
    "        # 폴드 내 모델 학습\n",
    "        model.fit(X_tr, y_tr)\n",
    "        train_fold_pred[valid_index, :] = model.predict(X_te).reshape(-1,1) # y_train 예측, 폴드 끝나면 concat해야함\n",
    "        test_pred[:, folder_counter] = model.predict(X_test_n) # y_test 예측, 폴드 끝나면 평균 낼거임\n",
    "        \n",
    "    test_pred_mean = np.mean(test_pred, axis=1).reshape(-1,1)\n",
    "    \n",
    "    return train_fold_pred, test_pred_mean # 하나의 모델에 대한 학습데이터, 테스트 데이터 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21520/53368132.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# 개별 모델로부터 메타 모델에 필요한 데이터 셋 만들기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mknn_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mknn_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stacking_base_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mknn_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mrf_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stacking_base_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrf_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mdt_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_stacking_base_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_21520/2660350131.py\u001b[0m in \u001b[0;36mget_stacking_base_datasets\u001b[1;34m(model, X_train_n, y_train_n, X_test_n, n_folds)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# 메타 모델을 위한 학습 및 테스트 데이터 만들기\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_stacking_base_datasets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test_n\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_folds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mkf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_folds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[1;31m# 빈 배열 생성\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtrain_fold_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_n\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    426\u001b[0m     def __init__(self, n_splits=5, *, shuffle=False,\n\u001b[0;32m    427\u001b[0m                  random_state=None):\n\u001b[1;32m--> 428\u001b[1;33m         super().__init__(n_splits=n_splits, shuffle=shuffle,\n\u001b[0m\u001b[0;32m    429\u001b[0m                          random_state=random_state)\n\u001b[0;32m    430\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, n_splits, shuffle, random_state)\u001b[0m\n\u001b[0;32m    288\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    289\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# None is the default\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 290\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    291\u001b[0m                 \u001b[1;34m'Setting a random_state has no effect since shuffle is '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[1;34m'False. You should leave '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Setting a random_state has no effect since shuffle is False. You should leave random_state to its default (None), or set shuffle=True."
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression # 메타 모델\n",
    "\n",
    "# 객체 생성\n",
    "knn_clf = KNeighborsClassifier()\n",
    "rf_clf = RandomForestClassifier()\n",
    "dt_clf = DecisionTreeClassifier()\n",
    "ada_clf = AdaBoostClassifier()\n",
    "lr_final = LogisticRegression()\n",
    "\n",
    "# 개별 모델로부터 메타 모델에 필요한 데이터 셋 만들기\n",
    "knn_train, knn_test = get_stacking_base_datasets(knn_clf, x_train, y_train, x_test, 7)\n",
    "rf_train, rf_test = get_stacking_base_datasets(rf_clf, x_train, y_train, x_test, 7)\n",
    "dt_train, dt_test = get_stacking_base_datasets(dt_clf, x_train, y_train, x_test, 7)\n",
    "ada_train, ada_test = get_stacking_base_datasets(ada_clf, x_train, y_train, x_test, 7)\n",
    "\n",
    "# 개별 모델로부터 나온 y_train 예측값들 옆으로 붙이기\n",
    "Stack_final_X_train = np.concatenate((knn_train,rf_train,dt_train,ada_train), axis=1)\n",
    "# 개별 모델로부터 나온 y_test 예측값들 옆으로 붙이기\n",
    "Stack_final_X_test = np.concatenate((knn_test,rf_test,dt_test,ada_test), axis=1)\n",
    "\n",
    "lr_final.fit(Stack_final_X_train, y_train)\n",
    "stack_final = lr_final.predict(Stack_final_X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 개별 모델 \n",
    "# from sklearn.svm import SVC \n",
    "# from sklearn.tree import RandomForestClassifier\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "\n",
    "# svm = SVC(random_state=0)\n",
    "# rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "# lr = LogisticRegression()\n",
    "\n",
    "# lgbm = LGBMClassifier"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "65c4fcde076dda63ccc6b64d0f319e611dd5685985abc4466de4eee25f4a3bfd"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
